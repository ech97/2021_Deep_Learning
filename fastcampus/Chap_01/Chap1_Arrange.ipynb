{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37864bittensorflowvenv10b5cbcd7cd74bb9aca26b9edce6abe3",
   "display_name": "Python 3.7.8 64-bit ('tensorflow': venv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 과정\n",
    "\n",
    "1. 데이터 가져오기\n",
    "2. 데이터 전처리\n",
    "\n",
    "3. 모델 구성\n",
    "    - Layer\n",
    "        - Convolution\n",
    "        > Output Ch., Activation Func., Pooling, Dropout\n",
    "\n",
    "        - Fully Connected (DNN)\n",
    "        > Flatten(Reshape), Dense(Output Ch.), Activation Func., Dropout\n",
    "        \n",
    "    - Param\n",
    "        - Loss\n",
    "            - (Sparse) Categorical CrossEntropy / Binary Cross Entropy\n",
    "            > 오차 계산 방식\n",
    "\n",
    "        - Optimizer\n",
    "            - SGD / RMS Prop / Adam\n",
    "            > 기여율 계산 방식\n",
    "        \n",
    "        - Metrics\n",
    "            > Accuracy 계산 방식 / (맞은 개수 / 전체 개수)\n",
    "            \n",
    "   - Compile\n",
    "    > 모델 구성에 Param 선언까지 해주기\n",
    "\n",
    "4. 모델 학습\n",
    "    - Hyper Parameter\n",
    "        - learning rate \n",
    "            > 계산된 오차, 기여율을 얼만큼 Scaling해서 Weight를 조정해줄지\n",
    "        - batch size \n",
    "            > 한번에 얼만큼의 이미지를 넣을것인지\n",
    "        - epoch \n",
    "            > 학습과정을 얼마나 반복할것인지\n",
    "\n",
    "5. 평가\n",
    "    - Test Data\n",
    "        - Back Propagation, Gradient 계산 불필요 (bcz. 단순 예측, 라벨 비교)\n",
    "        - argmax(dim, keepdim)\n",
    "            > Softmax 되어 나온 확률 중 가장 큰 값의 라벨 도출\n",
    "        "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Tensorflow\n",
    "--------------\n",
    "## 데이터 불러오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "from  tensorflow.keras import datasets, layers\n",
    "\n",
    "mnist = datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data(\"mnist.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "num_label = 10\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "source": [
    "## 모델 구성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution\n",
    "\n",
    "inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "net = layers.Conv2D(32, 3, padding='SAME')(inputs)\n",
    "# 출력 채널(width, num_filter) = 32\n",
    "# 커널 크기(filter) = (3, 3)\n",
    "net = layers.Activation('relu')(net)\n",
    "\n",
    "net = layers.Conv2D(32, 3, padding = \"SAME\")(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "\n",
    "net = layers.MaxPool2D((2, 2))(net) # (batch_size, 14, 14, 32)\n",
    "net = layers.Dropout(dropout_rate)(net)\n",
    "\n",
    "net = layers.Conv2D(64, 3, padding = \"SAME\")(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "\n",
    "net = layers.Conv2D(64, 3, padding = \"SAME\")(net)\n",
    "net = layers.Activation('relu')(net)\n",
    "\n",
    "net = layers.MaxPool2D((2, 2))(net) # (batch_size, 7, 7, 64)\n",
    "net = layers.Dropout(0.25)(net)\n",
    "\n",
    "\n",
    "\n",
    "# Fully Connected (DNN)\n",
    "\n",
    "net = layers.Flatten()(net)  # (1, 3136)\n",
    "net = layers.Dense(512)(net) # (1, 512)\n",
    "net = layers.Activation('relu')(net)\n",
    "\n",
    "net = layers.Dropout(dropout_rate)(net)\n",
    "\n",
    "net = layers.Dense(num_label)(net)\n",
    "net = layers.Activation('softmax')(net)\n",
    "\n",
    "\n",
    "# 모델 이름 설정\n",
    "model = tf.keras.Model(inputs=inputs, outputs=net, name='Basic_CNN')"
   ]
  },
  {
   "source": [
    "## Parameter 및 compile"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 1\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model.compile(optimizer, loss, metrics)"
   ]
  },
  {
   "source": [
    "## 데이터 전처리"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train[..., tf.newaxis], x_test[..., tf.newaxis]\n",
    "\n",
    "x_train, x_test = x_train / 255., x_test / 255."
   ]
  },
  {
   "source": [
    "## 모델 학습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "938/938 [==============================] - 187s 200ms/step - loss: 0.1858 - accuracy: 0.9413\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size, num_epochs, shuffle = True)"
   ]
  },
  {
   "source": [
    "## 모델 평가"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "model.evaluate(x_test, y_test, batch_size)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "157/157 [==============================] - 5s 34ms/step - loss: 0.0411 - accuracy: 0.9865\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.04110949486494064, 0.9865000247955322]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ]
  },
  {
   "source": [
    "# Pytorch\n",
    "-----------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "\n",
    "# 데이터 관리를 위한 라이브러리\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "lr = 0.001\n",
    "momentum = 0.5\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "log_interval = 100"
   ]
  },
  {
   "source": [
    "## 데이터 가져오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('dataset', train = True, download = True,\n",
    "                    transform = transforms.Compose([\n",
    "                        transforms.ToTensor(), # 데이터를 Tensor 형식으로\n",
    "                        transforms.Normalize((0.1307, ), (0.3081, )) # 데이터 표준화 \n",
    "                    ])),\n",
    "    batch_size = batch_size, shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('dataset', train=False,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307, ), (0.3081, ))])),\n",
    "    batch_size = test_batch_size, shuffle = True)"
   ]
  },
  {
   "source": [
    "## 모델 구성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1) #input ch., output ch., kernel size, stri\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4*4*50, 500) # (Torch) Linear = (TF) Dense\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) # 커널 생성 후, Activation 함수 적용\n",
    "        x = F.max_pool2d(x, 2, 2) # pool_size, stride\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        x = x.reshape(-1, 4 * 4 * 50)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net().to(device) # device에 모델 컴파일"
   ]
  },
  {
   "source": [
    "## (Torch)Optimizer 및 Compile\n",
    "(TF) Optimizer, Loss, metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "# model.parameters = 모델을 구성하고있는 층 별 shape를 확인할수있음 (Weight, Bias 등)"
   ]
  },
  {
   "source": [
    "## 모델 학습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\t Loss: 2.292215\n",
      "Train Epoch: 1 [6400/60000 (11%)]\t Loss: 2.231636\n",
      "Train Epoch: 1 [12800/60000 (21%)]\t Loss: 2.151003\n",
      "Train Epoch: 1 [19200/60000 (32%)]\t Loss: 1.913002\n",
      "Train Epoch: 1 [25600/60000 (43%)]\t Loss: 1.640912\n",
      "Train Epoch: 1 [32000/60000 (53%)]\t Loss: 1.259970\n",
      "Train Epoch: 1 [38400/60000 (64%)]\t Loss: 0.862546\n",
      "Train Epoch: 1 [44800/60000 (75%)]\t Loss: 0.898350\n",
      "Train Epoch: 1 [51200/60000 (85%)]\t Loss: 0.610933\n",
      "Train Epoch: 1 [57600/60000 (96%)]\t Loss: 0.739944\n",
      "\n",
      "Test set: Average loss: 0.0077, Accuracy: 8589/10000 (86%)n\n",
      "Train Epoch: 2 [0/60000 (0%)]\t Loss: 0.443154\n",
      "Train Epoch: 2 [6400/60000 (11%)]\t Loss: 0.533468\n",
      "Train Epoch: 2 [12800/60000 (21%)]\t Loss: 0.402233\n",
      "Train Epoch: 2 [19200/60000 (32%)]\t Loss: 0.407886\n",
      "Train Epoch: 2 [25600/60000 (43%)]\t Loss: 0.338484\n",
      "Train Epoch: 2 [32000/60000 (53%)]\t Loss: 0.377512\n",
      "Train Epoch: 2 [38400/60000 (64%)]\t Loss: 0.327293\n",
      "Train Epoch: 2 [44800/60000 (75%)]\t Loss: 0.432609\n",
      "Train Epoch: 2 [51200/60000 (85%)]\t Loss: 0.412013\n",
      "Train Epoch: 2 [57600/60000 (96%)]\t Loss: 0.463759\n",
      "\n",
      "Test set: Average loss: 0.0047, Accuracy: 9121/10000 (91%)n\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    model.train() # train모드로 변경\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target) # 오차 계산\n",
    "        loss.backward() # backward로 영향력 계산\n",
    "        optimizer.step() # 얼만큼 뺄지 계산\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100 * batch_idx / len(train_loader), loss.item()\n",
    "            ))\n",
    "    \n",
    "\n",
    "    model.eval() # evaluation 모드로 변경\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100 * correct / len(test_loader.dataset)\n",
    "    ))\n"
   ]
  },
  {
   "source": [
    "# Matplotlib 관련은 09.ipynb 확인"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}